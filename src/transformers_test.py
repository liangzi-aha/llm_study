import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# =====================================================
# ç›´æ¥æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨ä½ å·²ä¸‹è½½çš„ModelScopeç¼“å­˜ï¼‰
# =====================================================

# ä½ çš„ModelScopeä¸‹è½½è·¯å¾„
local_model_path = os.path.expanduser("~/.cache/modelscope/hub/models/Qwen/Qwen2.5-7B-Instruct")

print(f"ğŸ“‚ ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {local_model_path}")
print(f"ğŸ“ è·¯å¾„å­˜åœ¨å—? {os.path.exists(local_model_path)}")

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
print("\nğŸ“‹ æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:")
for file in os.listdir(local_model_path)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
    print(f"  - {file}")

# åŠ è½½åˆ†è¯å™¨ï¼ˆä»æœ¬åœ°ï¼‰
print("\nğŸ“ åŠ è½½åˆ†è¯å™¨...")
tokenizer = AutoTokenizer.from_pretrained(
    local_model_path,  # ç›´æ¥ç”¨æœ¬åœ°è·¯å¾„
    trust_remote_code=True
)

# åŠ è½½æ¨¡å‹ï¼ˆä»æœ¬åœ°ï¼‰
print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    local_model_path,  # ç›´æ¥ç”¨æœ¬åœ°è·¯å¾„
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="cpu",
    low_cpu_mem_usage=True
)

print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼è®¾å¤‡: {model.device}")

# æµ‹è¯•å¯¹è¯
messages = [{"role": "user", "content": "ä½ å¥½ï¼Œç®€å•ä»‹ç»ä¸‹è‡ªå·±"}]
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([input_text], return_tensors='pt')

print("â³ ç”Ÿæˆå›å¤ä¸­...")
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True
)

response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
print(f"\nğŸ¤– æ¨¡å‹å›å¤: {response}")